{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ea4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pygame\n",
    "# %pip install numpy\n",
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set a specific integer for reproducibility\n",
    "SEED_VALUE = 20250506\n",
    "\n",
    "# 1. Set seed for Python's built-in random module\n",
    "random.seed(SEED_VALUE)\n",
    "\n",
    "# 2. Set seed for NumPy operations\n",
    "np.random.seed(SEED_VALUE)\n",
    "\n",
    "# 3. Set seed for TensorFlow (affects weight initialization and some GPU operations)\n",
    "tf.random.set_seed(SEED_VALUE)\n",
    "# Note: You might also need to set some environment variables for complete determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdeb4fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Files\\School Files\\Reinforcement Learning\\Project\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "# ----------------\n",
    "# Constants\n",
    "# ----------------\n",
    "DISPLAY_INFO = pygame.display.Info()\n",
    "MONITOR_WIDTH = DISPLAY_INFO.current_w\n",
    "MONITOR_HEIGHT = DISPLAY_INFO.current_h\n",
    "\n",
    "SCREEN_WIDTH = int(MONITOR_WIDTH * 0.6)\n",
    "SCREEN_HEIGHT = int(MONITOR_HEIGHT * 0.8)\n",
    "\n",
    "GRID_SIZE = 8\n",
    "AVAILABLE_HEIGHT = SCREEN_HEIGHT - 230\n",
    "AVAILABLE_WIDTH = SCREEN_WIDTH - 40\n",
    "\n",
    "TILE_SIZE = min(AVAILABLE_HEIGHT // GRID_SIZE, AVAILABLE_WIDTH // GRID_SIZE)\n",
    "MARGIN = 150\n",
    "\n",
    "MINE_COUNT = 5\n",
    "FLAG_LIMIT = 5\n",
    "MAX_LIVES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c201a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Get display info and set screen size based on monitor\n",
    "display_info = pygame.display.Info()\n",
    "MONITOR_WIDTH = display_info.current_w\n",
    "MONITOR_HEIGHT = display_info.current_h\n",
    "\n",
    "# Set screen size to a percentage of monitor size\n",
    "SCREEN_WIDTH = int(MONITOR_WIDTH * 0.6)  # 60% of monitor width\n",
    "SCREEN_HEIGHT = int(MONITOR_HEIGHT * 0.8)  # 80% of monitor height\n",
    "\n",
    "# Adjust tile size based on screen size\n",
    "GRID_SIZE = 8\n",
    "AVAILABLE_HEIGHT = SCREEN_HEIGHT - 230  # Subtract margins (150 top + 80 bottom)\n",
    "AVAILABLE_WIDTH = SCREEN_WIDTH - 40  # Some padding\n",
    "\n",
    "# Calculate tile size to fit the grid\n",
    "TILE_SIZE = min(AVAILABLE_HEIGHT // GRID_SIZE, AVAILABLE_WIDTH // GRID_SIZE)\n",
    "MARGIN = 150\n",
    "\n",
    "# Game constants\n",
    "MINE_COUNT = 5\n",
    "FLAG_LIMIT = 5\n",
    "LIVES = 3\n",
    "\n",
    "# Colors\n",
    "BG_COLOR = (40, 44, 52)\n",
    "PANEL_COLOR = (30, 34, 42)\n",
    "UNREVEALED_COLOR = (58, 95, 135)\n",
    "UNREVEALED_HOVER_COLOR = (75, 115, 160)\n",
    "REVEALED_COLOR = (220, 220, 225)\n",
    "MINE_COLOR = (215, 95, 95)\n",
    "FLAG_COLOR = (255, 50, 50)\n",
    "BORDER_COLOR = (20, 20, 25)\n",
    "LIFE_COLOR = (255, 80, 80)\n",
    "\n",
    "# Number colors (classic minesweeper colors)\n",
    "NUMBER_COLORS = {\n",
    "    1: (45, 85, 165),\n",
    "    2: (60, 145, 70),\n",
    "    3: (185, 60, 60),\n",
    "    4: (35, 60, 135),\n",
    "    5: (150, 50, 50),\n",
    "    6: (50, 140, 140),\n",
    "    7: (40, 40, 40),\n",
    "    8: (100, 100, 100)\n",
    "}\n",
    "\n",
    "# Create resizable window\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT), pygame.RESIZABLE)\n",
    "pygame.display.set_caption(\"Minesweeper RL\")\n",
    "\n",
    "class Tile:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.is_mine = False\n",
    "        self.is_revealed = False\n",
    "        self.is_flagged = False\n",
    "        self.adjacent_mines = 0\n",
    "        self.hover = False\n",
    "\n",
    "class MinesweeperGame:\n",
    "    def __init__(self):\n",
    "        self.grid = [[Tile(x, y) for y in range(GRID_SIZE)] for x in range(GRID_SIZE)]\n",
    "        self.game_over = False\n",
    "        self.game_won = False\n",
    "        self.flags_placed = 0\n",
    "        self.flag_limit = FLAG_LIMIT\n",
    "        self.revealed_count = 0\n",
    "        self.score = 0\n",
    "        self.start_time = pygame.time.get_ticks()\n",
    "        self.end_time = None\n",
    "        self.combo = 0\n",
    "        self.max_combo = 0\n",
    "        self.lives = LIVES\n",
    "        self.total_mines = MINE_COUNT\n",
    "        self.mines_hit = []  # Track which mines were hit\n",
    "        self.setup_mines()\n",
    "        self.calculate_adjacent_mines()\n",
    "    \n",
    "    def setup_mines(self):\n",
    "        mines_placed = 0\n",
    "        while mines_placed < self.total_mines:\n",
    "            x = random.randint(0, GRID_SIZE - 1)\n",
    "            y = random.randint(0, GRID_SIZE - 1)\n",
    "            if not self.grid[x][y].is_mine:\n",
    "                self.grid[x][y].is_mine = True\n",
    "                mines_placed += 1\n",
    "    \n",
    "    def calculate_adjacent_mines(self):\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if not self.grid[x][y].is_mine:\n",
    "                    count = 0\n",
    "                    for dx in [-1, 0, 1]:\n",
    "                        for dy in [-1, 0, 1]:\n",
    "                            if dx == 0 and dy == 0:\n",
    "                                continue\n",
    "                            nx, ny = x + dx, y + dy\n",
    "                            if 0 <= nx < GRID_SIZE and 0 <= ny < GRID_SIZE:\n",
    "                                if self.grid[nx][ny].is_mine:\n",
    "                                    count += 1\n",
    "                    self.grid[x][y].adjacent_mines = count\n",
    "    \n",
    "    def reveal_tile(self, x, y):\n",
    "        if self.game_over or self.game_won:\n",
    "            return\n",
    "        \n",
    "        tile = self.grid[x][y]\n",
    "        if tile.is_revealed or tile.is_flagged:\n",
    "            return\n",
    "        \n",
    "        tile.is_revealed = True\n",
    "        self.revealed_count += 1\n",
    "        \n",
    "        if tile.is_mine:\n",
    "            self.lives -= 1\n",
    "            self.combo = 0  # Reset combo on mine hit\n",
    "            self.mines_hit.append((x, y))\n",
    "            \n",
    "            if self.lives <= 0:\n",
    "                self.game_over = True\n",
    "                self.end_time = pygame.time.get_ticks()\n",
    "                self.reveal_all_mines()\n",
    "            return\n",
    "        \n",
    "        # Add score based on tile value\n",
    "        if tile.adjacent_mines == 0:\n",
    "            self.score += 10\n",
    "            self.combo += 1\n",
    "        else:\n",
    "            self.score += tile.adjacent_mines * 5\n",
    "            self.combo += 1\n",
    "        \n",
    "        # Combo bonus\n",
    "        if self.combo > 5:\n",
    "            self.score += self.combo * 2\n",
    "        \n",
    "        self.max_combo = max(self.max_combo, self.combo)\n",
    "        \n",
    "        if tile.adjacent_mines == 0:\n",
    "            for dx in [-1, 0, 1]:\n",
    "                for dy in [-1, 0, 1]:\n",
    "                    if dx == 0 and dy == 0:\n",
    "                        continue\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if 0 <= nx < GRID_SIZE and 0 <= ny < GRID_SIZE:\n",
    "                        self.reveal_tile(nx, ny)\n",
    "        \n",
    "        # Check win condition: all non-mine tiles revealed\n",
    "        non_mine_tiles = GRID_SIZE * GRID_SIZE - self.total_mines\n",
    "        safe_tiles_revealed = self.revealed_count - len(self.mines_hit)\n",
    "        \n",
    "        if safe_tiles_revealed == non_mine_tiles:\n",
    "            self.game_won = True\n",
    "            self.end_time = pygame.time.get_ticks()\n",
    "            # Win bonus\n",
    "            time_bonus = max(0, 1000 - (self.end_time - self.start_time) // 100)\n",
    "            self.score += time_bonus\n",
    "            # Perfect flag bonus\n",
    "            if self.flags_placed == MINE_COUNT:\n",
    "                self.score += 500\n",
    "            # Lives bonus\n",
    "            self.score += self.lives * 200\n",
    "    \n",
    "    def toggle_flag(self, x, y):\n",
    "        if self.game_over or self.game_won:\n",
    "            return\n",
    "        tile = self.grid[x][y]\n",
    "        if not tile.is_revealed:\n",
    "            if tile.is_flagged:\n",
    "                tile.is_flagged = False\n",
    "                self.flags_placed -= 1\n",
    "                self.combo = 0  # Reset combo on flag remove\n",
    "                # Cancel bonus for removing a correct flag\n",
    "                if tile.is_mine:\n",
    "                    self.score -= 50\n",
    "            elif self.flags_placed < self.flag_limit:\n",
    "                tile.is_flagged = True\n",
    "                self.flags_placed += 1\n",
    "                # Bonus for correct flag\n",
    "                if tile.is_mine:\n",
    "                    self.score += 50\n",
    "                self.combo = 0  # Reset combo on flag\n",
    "    \n",
    "    def reveal_all_mines(self):\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if self.grid[x][y].is_mine:\n",
    "                    self.grid[x][y].is_revealed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58080ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from A2C import A2C  # Assuming A2C is defined in A2C.py\n",
    "\n",
    "def get_tile_from_mouse(mx, my):\n",
    "    if my < MARGIN or my >= MARGIN + GRID_SIZE * TILE_SIZE:\n",
    "        return None, None\n",
    "    if mx < (SCREEN_WIDTH - GRID_SIZE * TILE_SIZE) // 2:\n",
    "        return None, None\n",
    "    if mx >= (SCREEN_WIDTH - GRID_SIZE * TILE_SIZE) // 2 + GRID_SIZE * TILE_SIZE:\n",
    "        return None, None\n",
    "    \n",
    "    offset_x = (SCREEN_WIDTH - GRID_SIZE * TILE_SIZE) // 2\n",
    "    grid_x = (mx - offset_x) // TILE_SIZE\n",
    "    grid_y = (my - MARGIN) // TILE_SIZE\n",
    "    \n",
    "    if 0 <= grid_x < GRID_SIZE and 0 <= grid_y < GRID_SIZE:\n",
    "        return int(grid_x), int(grid_y)\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def draw_game(game, agent: Optional[A2C] = None):\n",
    "    screen.fill(BG_COLOR)\n",
    "    \n",
    "    # Draw top panel\n",
    "    pygame.draw.rect(screen, PANEL_COLOR, (0, 0, SCREEN_WIDTH, MARGIN))\n",
    "    \n",
    "    # Draw bottom panel\n",
    "    pygame.draw.rect(screen, PANEL_COLOR, (0, SCREEN_HEIGHT - 80, SCREEN_WIDTH, 80))\n",
    "    \n",
    "    # Calculate grid offset for centering\n",
    "    offset_x = (SCREEN_WIDTH - GRID_SIZE * TILE_SIZE) // 2\n",
    "    offset_y = MARGIN\n",
    "    \n",
    "    # Draw grid\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            tile = game.grid[x][y]\n",
    "            rect = pygame.Rect(offset_x + x * TILE_SIZE, offset_y + y * TILE_SIZE, TILE_SIZE, TILE_SIZE)\n",
    "            \n",
    "            # Draw border\n",
    "            pygame.draw.rect(screen, BORDER_COLOR, rect, 3)\n",
    "            \n",
    "            # Shrink rect for inner tile\n",
    "            inner_rect = rect.inflate(-6, -6)\n",
    "            \n",
    "            if tile.is_revealed:\n",
    "                if tile.is_mine:\n",
    "                    pygame.draw.rect(screen, MINE_COLOR, inner_rect)\n",
    "                    # Draw mine circle\n",
    "                    center = inner_rect.center\n",
    "                    pygame.draw.circle(screen, (40, 40, 40), center, 20)\n",
    "                    # Draw X if this mine was hit\n",
    "                    if (x, y) in game.mines_hit:\n",
    "                        pygame.draw.line(screen, (255, 255, 255), \n",
    "                                       (center[0] - 12, center[1] - 12), \n",
    "                                       (center[0] + 12, center[1] + 12), 4)\n",
    "                        pygame.draw.line(screen, (255, 255, 255), \n",
    "                                       (center[0] + 12, center[1] - 12), \n",
    "                                       (center[0] - 12, center[1] + 12), 4)\n",
    "                else:\n",
    "                    pygame.draw.rect(screen, REVEALED_COLOR, inner_rect)\n",
    "                    if tile.adjacent_mines > 0:\n",
    "                        # Draw number\n",
    "                        font = pygame.font.Font(None, 60)\n",
    "                        text = font.render(str(tile.adjacent_mines), True, NUMBER_COLORS[tile.adjacent_mines])\n",
    "                        text_rect = text.get_rect(center=inner_rect.center)\n",
    "                        screen.blit(text, text_rect)\n",
    "            else:\n",
    "                if tile.hover:\n",
    "                    pygame.draw.rect(screen, UNREVEALED_HOVER_COLOR, inner_rect)\n",
    "                else:\n",
    "                    pygame.draw.rect(screen, UNREVEALED_COLOR, inner_rect)\n",
    "                \n",
    "                if tile.is_flagged:\n",
    "                    # Draw flag triangle\n",
    "                    center_x, center_y = inner_rect.center\n",
    "                    # Flag pole\n",
    "                    pygame.draw.rect(screen, (60, 60, 60), (center_x - 2, center_y - 20, 4, 35))\n",
    "                    # Flag triangle\n",
    "                    flag_points = [\n",
    "                        (center_x + 2, center_y - 18),\n",
    "                        (center_x + 22, center_y - 8),\n",
    "                        (center_x + 2, center_y + 2)\n",
    "                    ]\n",
    "                    pygame.draw.polygon(screen, FLAG_COLOR, flag_points)\n",
    "                    pygame.draw.polygon(screen, (180, 30, 30), flag_points, 2)\n",
    "\n",
    "    # Visualize Last Agent Action\n",
    "    if agent is not None and agent.LAST_ACTION is not None:\n",
    "        x, y, action_type = agent.LAST_ACTION\n",
    "        \n",
    "        offset_x = (SCREEN_WIDTH - GRID_SIZE * TILE_SIZE) // 2\n",
    "        offset_y = MARGIN\n",
    "        \n",
    "        action_x = offset_x + x * TILE_SIZE + TILE_SIZE // 2\n",
    "        action_y = offset_y + y * TILE_SIZE + TILE_SIZE // 2\n",
    "        \n",
    "        # Blue circle for reveal, red for flag\n",
    "        color = (100, 200, 255) if action_type == 0 else (255, 100, 100)\n",
    "        pygame.draw.circle(screen, color, (action_x, action_y), TILE_SIZE // 2, 5)\n",
    "    \n",
    "    # Draw UI text\n",
    "    font_large = pygame.font.Font(None, 48)\n",
    "    font_medium = pygame.font.Font(None, 36)\n",
    "    font_small = pygame.font.Font(None, 28)\n",
    "    font_xsmall = pygame.font.Font(None, 20)\n",
    "    \n",
    "    # Calculate time\n",
    "    if game.end_time:\n",
    "        elapsed = (game.end_time - game.start_time) // 1000\n",
    "    else:\n",
    "        elapsed = (pygame.time.get_ticks() - game.start_time) // 1000\n",
    "    \n",
    "    if game.game_over:\n",
    "        text = font_large.render(\"GAME OVER!\", True, (235, 100, 95))\n",
    "        screen.blit(text, (SCREEN_WIDTH // 2 - text.get_width() // 2, 30))\n",
    "        score_text = font_medium.render(f\"Final Score: {game.score}\", True, (255, 200, 100))\n",
    "        screen.blit(score_text, (SCREEN_WIDTH // 2 - score_text.get_width() // 2, 80))\n",
    "    elif game.game_won:\n",
    "        text = font_large.render(\"YOU WIN!\", True, (90, 200, 110))\n",
    "        screen.blit(text, (SCREEN_WIDTH // 2 - text.get_width() // 2, 30))\n",
    "        score_text = font_medium.render(f\"Final Score: {game.score}\", True, (100, 255, 150))\n",
    "        screen.blit(score_text, (SCREEN_WIDTH // 2 - score_text.get_width() // 2, 80))\n",
    "        \n",
    "        # Show achievements\n",
    "        achievements = []\n",
    "        if elapsed < 60:\n",
    "            achievements.append(\"âš¡ Speed Demon!\")\n",
    "        if game.max_combo >= 10:\n",
    "            achievements.append(\"ðŸ”¥ Combo Master!\")\n",
    "        if game.flags_placed == MINE_COUNT:\n",
    "            achievements.append(\"ðŸŽ¯ Perfect Flags!\")\n",
    "        if game.lives == LIVES:\n",
    "            achievements.append(\"ðŸ’Ž Flawless Victory!\")\n",
    "        \n",
    "        y_pos = 120\n",
    "        for achievement in achievements:\n",
    "            ach_text = font_small.render(achievement, True, (255, 215, 0))\n",
    "            screen.blit(ach_text, (SCREEN_WIDTH // 2 - ach_text.get_width() // 2, y_pos))\n",
    "            y_pos += 30\n",
    "    else:\n",
    "        # Top left - Stats\n",
    "        text1 = font_medium.render(f\"Score: {game.score}\", True, (255, 215, 0))\n",
    "        text2 = font_medium.render(f\"Time: {elapsed}s\", True, (100, 200, 255))\n",
    "        screen.blit(text1, (30, 30))\n",
    "        screen.blit(text2, (30, 65))\n",
    "        \n",
    "        # Draw lives as hearts\n",
    "        heart_y = 100\n",
    "        for i in range(LIVES):\n",
    "            heart_x = 40 + i * 35\n",
    "            if i < game.lives:\n",
    "                # Filled heart\n",
    "                color = LIFE_COLOR\n",
    "            else:\n",
    "                # Empty heart\n",
    "                color = (80, 80, 80)\n",
    "            \n",
    "            # Draw simple heart shape\n",
    "            pygame.draw.circle(screen, color, (heart_x, heart_y), 10)\n",
    "            pygame.draw.circle(screen, color, (heart_x + 14, heart_y), 10)\n",
    "            heart_points = [\n",
    "                (heart_x - 10, heart_y + 3),\n",
    "                (heart_x + 7, heart_y + 20),\n",
    "                (heart_x + 24, heart_y + 3)\n",
    "            ]\n",
    "            pygame.draw.polygon(screen, color, heart_points)\n",
    "        \n",
    "        # Top right - Game info\n",
    "        mines_left = MINE_COUNT - game.flags_placed\n",
    "        text3 = font_medium.render(f\"Mines: {mines_left}\", True, (240, 200, 120))\n",
    "        text4 = font_medium.render(f\"Flags: {game.flags_placed}/{FLAG_LIMIT}\", True, (120, 180, 230))\n",
    "        screen.blit(text3, (SCREEN_WIDTH - 200, 30))\n",
    "        screen.blit(text4, (SCREEN_WIDTH - 200, 65))\n",
    "        \n",
    "        # Combo indicator\n",
    "        if game.combo > 3:\n",
    "            combo_text = font_medium.render(f\"Combo x{game.combo}!\", True, (255, 100, 255))\n",
    "            yPos = 30 if agent is None else 5\n",
    "            screen.blit(combo_text, (SCREEN_WIDTH // 2 - combo_text.get_width() // 2, yPos))\n",
    "\n",
    "    # Display metrics for RL agent\n",
    "    if agent is not None:\n",
    "        metrics_y = 36\n",
    "        metrics = [\n",
    "            agent.printMetrics([\"Win Rate\"], True),\n",
    "            agent.printMetrics([\"Episodes\", \"Current Episode\"], True),\n",
    "            agent.printMetrics([\"Highest Score\"], True),\n",
    "            agent.printMetrics([\"Time Efficiency\"], True),\n",
    "            agent.printMetrics([\"Left Click Avg\", \"Right Click Avg\", ], True),\n",
    "            agent.printMetrics([\"Total Click Avg\"], True),\n",
    "            agent.printMetrics([\"Game Conclusion History\"], True)\n",
    "        ]\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_text = font_xsmall.render(metric, True, (200, 200, 200))\n",
    "            screen.blit(metric_text, (SCREEN_WIDTH // 2 - metric_text.get_width() // 2, metrics_y))\n",
    "            metrics_y += metric_text.get_height()\n",
    "    \n",
    "    # Bottom instructions\n",
    "    text1 = font_small.render(\"Left Click: Reveal\", True, (200, 200, 200))\n",
    "    text2 = font_small.render(\"Right Click: Flag\", True, (200, 200, 200))\n",
    "    text3 = font_small.render(\"R: Restart\", True, (200, 200, 200))\n",
    "\n",
    "    # Mouse control hints\n",
    "    if agent is not None:\n",
    "        text4 = font_small.render(\"Blue: Reveal\", True, (100, 200, 255))\n",
    "        text5 = font_small.render(\"Red: Flag\", True, (255, 100, 100))\n",
    "    \n",
    "    screen.blit(text1, (30, SCREEN_HEIGHT - 50))\n",
    "    screen.blit(text2, (250, SCREEN_HEIGHT - 50))\n",
    "    screen.blit(text3, (450, SCREEN_HEIGHT - 50))\n",
    "    screen.blit(text4, (SCREEN_WIDTH - 150, SCREEN_HEIGHT - 75))\n",
    "    screen.blit(text5, (SCREEN_WIDTH - 150, SCREEN_HEIGHT - 25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e3aee",
   "metadata": {},
   "source": [
    "# Key Implementation Changes\n",
    "\n",
    "The initial proposal provided a strong foundation. The following changes were implemented during development to improve the training effectiveness and final performance of the Actor-Critic agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08989d",
   "metadata": {},
   "source": [
    "## Enhanced Agent State ($S_t$)\n",
    "\n",
    "The agent's state representation was enhanced to provide a richer understanding of the environment. This allows the agent to make more intelligent decisions and prevents it from getting stuck in unproductive loops.\n",
    "\n",
    "The original proposed state included:\n",
    "\n",
    "- Visible Grid\n",
    "- Score\n",
    "- Lives\n",
    "- Flags\n",
    "- Combo\n",
    "- Game Status (active/won/lost)\n",
    "\n",
    "The final implementation was updated to these:\n",
    "\n",
    "- **Visible Grid:** The board state.\n",
    "- **Lives:** Tracks how many lives are left in the episode.\n",
    "- **Combo:** Tracks the real-time combo in the run.\n",
    "- **Flags:** Tracks how many flags are left allowed to be place on the board.\n",
    "- **Progress:** Tracks the ratio of revealed safe tiles to the total number of safe tiles.\n",
    "- **Interaction History:** Encodes how recently the agent has interacted with each tiles. Basically a history of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32805f33",
   "metadata": {},
   "source": [
    "## Simplified Reward Function ($R_t$)\n",
    "\n",
    "The initial reward function, tied to the complex in-game score, proved to be a noisy signal that limited the agent's performance (resulting in a 5-13% win rate). Therefore, it was replaced with a simplified, sparse reward function to provide a clearer learning objective.\n",
    "\n",
    "The new function is based on four simple rules:\n",
    "\n",
    "- **Win Bonus:** A large positive reward of **+100** is given when the agent successfully clears all non-mine tiles. This defines the ultimate goal.\n",
    "- **Loss Penalty:** A large negative penalty of **-100** is given when the agent loses its last life or runs out of time. This clearly punishes failure.\n",
    "- **Progress Reward:** A small positive reward of **+1 is given for each new safe tile** the agent successfully reveals in a single turn. This encourages the agent to explore and clear the board.\n",
    "- **Efficiency Penalty:** A tiny negative penalty of **-0.1 is applied for every action taken**. This incentivizes the agent to win in as few steps as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1b05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 1, Step 10] Score: 2440 | Reward: -100.00 | TD Error: -10.00 | Action: (6, 2) Reveal | Revealed: 49/59\n",
      "\n",
      "============================================================\n",
      "Episode 1 finished: LOSS | Score: 2440\n",
      "Episodes: 1 | Win Rate: 0.00% | Highest Score: 2440.00 | Time Efficiency: 4.38 steps/sec | Left Click Avg: 5.00 c/g (2.19 c/s) | Right Click Avg: 5.00 c/g (2.19 c/s) | Total Click Avg: 10.00 c/g (4.38 c/s) | Game Conclusion History: 0 Wins, 1 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 2, Step 10] Score: 1706 | Reward: -0.10 | TD Error: 0.33 | Action: (1, 1) Flag | Revealed: 50/59\n",
      "[Ep 2, Step 15] Score: 1711 | Reward: -100.00 | TD Error: -10.00 | Action: (0, 3) Reveal | Revealed: 53/59\n",
      "\n",
      "============================================================\n",
      "Episode 2 finished: LOSS | Score: 1711\n",
      "Episodes: 2 | Win Rate: 0.00% | Highest Score: 2440.00 | Time Efficiency: 5.33 steps/sec | Left Click Avg: 7.50 c/g (3.20 c/s) | Right Click Avg: 5.00 c/g (2.13 c/s) | Total Click Avg: 12.50 c/g (5.33 c/s) | Game Conclusion History: 0 Wins, 2 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 3, Step 10] Score: 3017 | Reward: -0.10 | TD Error: -0.09 | Action: (7, 7) Flag | Revealed: 57/59\n",
      "[Ep 3, Step 20] Score: 2972 | Reward: -0.10 | TD Error: 0.07 | Action: (7, 7) Flag | Revealed: 60/59\n",
      "[Ep 3, Step 26] Score: 4306 | Reward: 100.00 | TD Error: 10.00 | Action: (6, 7) Reveal | Revealed: 61/59\n",
      "\n",
      "============================================================\n",
      "Episode 3 finished: WIN | Score: 4306\n",
      "Episodes: 3 | Win Rate: 33.33% | Highest Score: 4306.00 | Time Efficiency: 6.41 steps/sec | Left Click Avg: 7.33 c/g (2.76 c/s) | Right Click Avg: 9.67 c/g (3.64 c/s) | Total Click Avg: 17.00 c/g (6.41 c/s) | Game Conclusion History: 1 Wins, 2 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 4, Step 10] Score: 3470 | Reward: -0.10 | TD Error: -0.50 | Action: (7, 0) Reveal | Revealed: 58/59\n",
      "[Ep 4, Step 12] Score: 3420 | Reward: -100.00 | TD Error: -10.00 | Action: (5, 3) Reveal | Revealed: 59/59\n",
      "\n",
      "============================================================\n",
      "Episode 4 finished: LOSS | Score: 3420\n",
      "Episodes: 4 | Win Rate: 25.00% | Highest Score: 4306.00 | Time Efficiency: 6.27 steps/sec | Left Click Avg: 7.25 c/g (2.89 c/s) | Right Click Avg: 8.50 c/g (3.38 c/s) | Total Click Avg: 15.75 c/g (6.27 c/s) | Game Conclusion History: 1 Wins, 3 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 5, Step 10] Score: 3560 | Reward: -0.10 | TD Error: 0.13 | Action: (2, 7) Flag | Revealed: 59/59\n",
      "[Ep 5, Step 15] Score: 3560 | Reward: -100.00 | TD Error: -10.00 | Action: (1, 2) Reveal | Revealed: 60/59\n",
      "\n",
      "============================================================\n",
      "Episode 5 finished: LOSS | Score: 3560\n",
      "Episodes: 5 | Win Rate: 20.00% | Highest Score: 4306.00 | Time Efficiency: 6.31 steps/sec | Left Click Avg: 7.00 c/g (2.83 c/s) | Right Click Avg: 8.60 c/g (3.48 c/s) | Total Click Avg: 15.60 c/g (6.31 c/s) | Game Conclusion History: 1 Wins, 4 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 6, Step 10] Score: 2023 | Reward: -0.10 | TD Error: 0.66 | Action: (2, 2) Flag | Revealed: 53/59\n",
      "[Ep 6, Step 20] Score: 2028 | Reward: -0.10 | TD Error: -0.38 | Action: (7, 7) Flag | Revealed: 55/59\n",
      "[Ep 6, Step 30] Score: 2028 | Reward: -0.10 | TD Error: -0.11 | Action: (7, 7) Flag | Revealed: 56/59\n",
      "[Ep 6, Step 40] Score: 2028 | Reward: -0.10 | TD Error: 0.08 | Action: (7, 7) Flag | Revealed: 56/59\n",
      "[Ep 6, Step 50] Score: 2028 | Reward: -0.10 | TD Error: 0.08 | Action: (7, 7) Flag | Revealed: 56/59\n",
      "[Ep 6, Step 60] Score: 1988 | Reward: -100.00 | TD Error: -10.00 | Action: (4, 5) Reveal | Revealed: 58/59\n",
      "\n",
      "============================================================\n",
      "Episode 6 finished: LOSS | Score: 1988\n",
      "Episodes: 6 | Win Rate: 16.67% | Highest Score: 4306.00 | Time Efficiency: 7.41 steps/sec | Left Click Avg: 7.00 c/g (2.25 c/s) | Right Click Avg: 16.00 c/g (5.15 c/s) | Total Click Avg: 23.00 c/g (7.41 c/s) | Game Conclusion History: 1 Wins, 5 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 7, Step 10] Score: 3367 | Reward: -0.10 | TD Error: 0.16 | Action: (7, 1) Flag | Revealed: 55/59\n",
      "[Ep 7, Step 13] Score: 3367 | Reward: -100.00 | TD Error: -10.00 | Action: (1, 6) Reveal | Revealed: 58/59\n",
      "\n",
      "============================================================\n",
      "Episode 7 finished: LOSS | Score: 3367\n",
      "Episodes: 7 | Win Rate: 14.29% | Highest Score: 4306.00 | Time Efficiency: 7.25 steps/sec | Left Click Avg: 6.86 c/g (2.30 c/s) | Right Click Avg: 14.71 c/g (4.95 c/s) | Total Click Avg: 21.57 c/g (7.25 c/s) | Game Conclusion History: 1 Wins, 6 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 8, Step 10] Score: 2161 | Reward: -0.10 | TD Error: 2.47 | Action: (3, 7) Flag | Revealed: 56/59\n",
      "[Ep 8, Step 20] Score: 2186 | Reward: -0.10 | TD Error: 0.70 | Action: (3, 1) Flag | Revealed: 58/59\n",
      "[Ep 8, Step 25] Score: 3621 | Reward: 100.00 | TD Error: 10.00 | Action: (3, 0) Reveal | Revealed: 60/59\n",
      "\n",
      "============================================================\n",
      "Episode 8 finished: WIN | Score: 3621\n",
      "Episodes: 8 | Win Rate: 25.00% | Highest Score: 4306.00 | Time Efficiency: 7.35 steps/sec | Left Click Avg: 6.75 c/g (2.25 c/s) | Right Click Avg: 15.25 c/g (5.09 c/s) | Total Click Avg: 22.00 c/g (7.35 c/s) | Game Conclusion History: 2 Wins, 6 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 9, Step 7] Score: 3420 | Reward: -100.00 | TD Error: -10.00 | Action: (5, 0) Reveal | Revealed: 58/59\n",
      "\n",
      "============================================================\n",
      "Episode 9 finished: LOSS | Score: 3420\n",
      "Episodes: 9 | Win Rate: 22.22% | Highest Score: 4306.00 | Time Efficiency: 7.16 steps/sec | Left Click Avg: 6.56 c/g (2.31 c/s) | Right Click Avg: 13.78 c/g (4.85 c/s) | Total Click Avg: 20.33 c/g (7.16 c/s) | Game Conclusion History: 2 Wins, 7 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 10, Step 10] Score: 3072 | Reward: 0.90 | TD Error: 0.36 | Action: (7, 7) Reveal | Revealed: 52/59\n",
      "[Ep 10, Step 20] Score: 3072 | Reward: -0.10 | TD Error: 0.60 | Action: (5, 1) Flag | Revealed: 53/59\n",
      "[Ep 10, Step 30] Score: 3237 | Reward: -0.10 | TD Error: -0.28 | Action: (7, 1) Flag | Revealed: 55/59\n",
      "[Ep 10, Step 40] Score: 3137 | Reward: -0.10 | TD Error: 4.65 | Action: (3, 2) Flag | Revealed: 55/59\n",
      "[Ep 10, Step 50] Score: 3147 | Reward: -0.10 | TD Error: -0.55 | Action: (5, 0) Flag | Revealed: 56/59\n",
      "[Ep 10, Step 60] Score: 3202 | Reward: -0.10 | TD Error: -0.42 | Action: (7, 1) Flag | Revealed: 57/59\n",
      "[Ep 10, Step 70] Score: 3152 | Reward: -0.10 | TD Error: -0.39 | Action: (5, 0) Flag | Revealed: 57/59\n",
      "[Ep 10, Step 80] Score: 3202 | Reward: -0.10 | TD Error: 0.19 | Action: (7, 1) Flag | Revealed: 57/59\n",
      "[Ep 10, Step 88] Score: 3162 | Reward: -100.00 | TD Error: -10.00 | Action: (3, 3) Reveal | Revealed: 60/59\n",
      "\n",
      "============================================================\n",
      "Episode 10 finished: LOSS | Score: 3162\n",
      "Episodes: 10 | Win Rate: 20.00% | Highest Score: 4306.00 | Time Efficiency: 7.63 steps/sec | Left Click Avg: 7.00 c/g (1.97 c/s) | Right Click Avg: 20.10 c/g (5.66 c/s) | Total Click Avg: 27.10 c/g (7.63 c/s) | Game Conclusion History: 2 Wins, 8 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 11, Step 10] Score: 2825 | Reward: -0.10 | TD Error: -3.97 | Action: (4, 6) Reveal | Revealed: 51/59\n",
      "[Ep 11, Step 20] Score: 2860 | Reward: 0.90 | TD Error: -0.15 | Action: (6, 6) Reveal | Revealed: 56/59\n",
      "[Ep 11, Step 30] Score: 2910 | Reward: -0.10 | TD Error: 1.30 | Action: (6, 7) Flag | Revealed: 57/59\n",
      "[Ep 11, Step 38] Score: 2910 | Reward: -100.00 | TD Error: -10.00 | Action: (7, 6) Reveal | Revealed: 58/59\n",
      "\n",
      "============================================================\n",
      "Episode 11 finished: LOSS | Score: 2910\n",
      "Episodes: 11 | Win Rate: 18.18% | Highest Score: 4306.00 | Time Efficiency: 7.75 steps/sec | Left Click Avg: 7.36 c/g (2.03 c/s) | Right Click Avg: 20.73 c/g (5.72 c/s) | Total Click Avg: 28.09 c/g (7.75 c/s) | Game Conclusion History: 2 Wins, 9 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 12, Step 8] Score: 3370 | Reward: -100.00 | TD Error: -10.00 | Action: (5, 5) Reveal | Revealed: 57/59\n",
      "\n",
      "============================================================\n",
      "Episode 12 finished: LOSS | Score: 3370\n",
      "Episodes: 12 | Win Rate: 16.67% | Highest Score: 4306.00 | Time Efficiency: 7.61 steps/sec | Left Click Avg: 7.08 c/g (2.04 c/s) | Right Click Avg: 19.33 c/g (5.57 c/s) | Total Click Avg: 26.42 c/g (7.61 c/s) | Game Conclusion History: 2 Wins, 10 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 13, Step 10] Score: 15 | Reward: -0.10 | TD Error: 8.28 | Action: (1, 7) Flag | Revealed: 2/59\n",
      "[Ep 13, Step 20] Score: 2950 | Reward: -0.10 | TD Error: 2.55 | Action: (1, 7) Flag | Revealed: 55/59\n",
      "[Ep 13, Step 30] Score: 3050 | Reward: -0.10 | TD Error: -2.63 | Action: (5, 7) Flag | Revealed: 55/59\n",
      "[Ep 13, Step 40] Score: 2950 | Reward: -0.10 | TD Error: 4.48 | Action: (1, 7) Flag | Revealed: 55/59\n",
      "[Ep 13, Step 50] Score: 3060 | Reward: -0.10 | TD Error: 4.13 | Action: (5, 7) Flag | Revealed: 58/59\n",
      "[Ep 13, Step 60] Score: 3060 | Reward: -0.10 | TD Error: 5.24 | Action: (5, 7) Flag | Revealed: 58/59\n",
      "[Ep 13, Step 70] Score: 3060 | Reward: -0.10 | TD Error: 5.35 | Action: (5, 7) Flag | Revealed: 58/59\n",
      "[Ep 13, Step 80] Score: 3010 | Reward: -0.10 | TD Error: 4.98 | Action: (5, 7) Flag | Revealed: 58/59\n",
      "[Ep 13, Step 90] Score: 3010 | Reward: -0.10 | TD Error: 3.96 | Action: (5, 7) Flag | Revealed: 58/59\n",
      "[Ep 13, Step 97] Score: 2960 | Reward: -100.00 | TD Error: -10.00 | Action: (4, 2) Reveal | Revealed: 59/59\n",
      "\n",
      "============================================================\n",
      "Episode 13 finished: LOSS | Score: 2960\n",
      "Episodes: 13 | Win Rate: 15.38% | Highest Score: 4306.00 | Time Efficiency: 7.64 steps/sec | Left Click Avg: 7.31 c/g (1.75 c/s) | Right Click Avg: 24.54 c/g (5.89 c/s) | Total Click Avg: 31.85 c/g (7.64 c/s) | Game Conclusion History: 2 Wins, 11 Losses\n",
      "============================================================\n",
      "\n",
      "[Ep 14, Step 10] Score: 2727 | Reward: -0.10 | TD Error: 0.46 | Action: (5, 0) Flag | Revealed: 49/59\n",
      "[Ep 14, Step 20] Score: 2727 | Reward: -0.10 | TD Error: -0.09 | Action: (5, 0) Flag | Revealed: 49/59\n",
      "[Ep 14, Step 30] Score: 2727 | Reward: -0.10 | TD Error: -0.07 | Action: (5, 0) Flag | Revealed: 49/59\n",
      "[Ep 14, Step 40] Score: 2727 | Reward: -0.10 | TD Error: -0.14 | Action: (5, 0) Flag | Revealed: 49/59\n",
      "[Ep 14, Step 50] Score: 2727 | Reward: -0.10 | TD Error: -0.13 | Action: (5, 0) Flag | Revealed: 49/59\n",
      "[Ep 14, Step 60] Score: 2727 | Reward: -0.10 | TD Error: -0.11 | Action: (5, 0) Flag | Revealed: 49/59\n",
      "[Ep 14, Step 70] Score: 2727 | Reward: -0.10 | TD Error: -0.11 | Action: (5, 0) Flag | Revealed: 49/59\n"
     ]
    }
   ],
   "source": [
    "from A2C import A2C\n",
    "import numpy as np\n",
    "\n",
    "game = None\n",
    "agent = None\n",
    "\n",
    "def main():\n",
    "    global SCREEN_WIDTH, SCREEN_HEIGHT, TILE_SIZE, screen, game, agent\n",
    "\n",
    "    game = MinesweeperGame()\n",
    "    agent = A2C(\n",
    "        gridSize = GRID_SIZE,\n",
    "        maxLives = LIVES,\n",
    "        gameInstance = game,\n",
    "        discountFactor = 0.999,\n",
    "        betaEntropy = 0.015,\n",
    "        lr = {\n",
    "            0: 0.001,\n",
    "            2000: 0.0005,\n",
    "            5000: 0.0001,\n",
    "\t\t}\n",
    "    )\n",
    "    \n",
    "    clock = pygame.time.Clock()\n",
    "    running = True\n",
    "\n",
    "    def clickHandler():\n",
    "        mx, my = pygame.mouse.get_pos()\n",
    "        gridX, gridY = get_tile_from_mouse(mx, my)\n",
    "            \n",
    "        if gridX is not None:\n",
    "            if event.button == 1:  # Left click\n",
    "                game.reveal_tile(gridX, gridY)\n",
    "            elif event.button == 3:  # Right click\n",
    "                game.toggle_flag(gridX, gridY)\n",
    "        return\n",
    "\n",
    "    def agentTurnLogic():\n",
    "        global game, agent\n",
    "\n",
    "        done = False\n",
    "        elapsedTime = ((pygame.time.get_ticks() - game.start_time) / 1000)\n",
    "        \n",
    "        if not (game.game_over or game.game_won):\n",
    "            \n",
    "            # Encode Current State\n",
    "            stateCurrent = agent._encodeState()\n",
    "            \n",
    "            # Agent Selects Action\n",
    "            x, y, actionType, actionIdx, _ = agent.chooseAction(stateCurrent)\n",
    "            \n",
    "            # Update action history and interaction tracking\n",
    "            agent.ACTION_HISTORY.append((x, y, actionType))\n",
    "            if len(agent.ACTION_HISTORY) > agent.ACTION_HISTORY_SIZE:\n",
    "                agent.ACTION_HISTORY.pop(0)\n",
    "            \n",
    "            agent.TILE_INTERACTION_HISTORY[(x, y)] = len(agent.ACTION_HISTORY)\n",
    "            agent.LAST_ACTION = (x, y, actionType)\n",
    "            \n",
    "            # Store pre-action state to measure outcome\n",
    "            livesOld = game.lives\n",
    "            revealedOld = game.revealed_count\n",
    "            reward = 0\n",
    "\n",
    "            # Execute Action\n",
    "            if actionType == 0:\n",
    "                game.reveal_tile(x, y)\n",
    "            else:\n",
    "                game.toggle_flag(x, y)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Simplified Reward Calculation\n",
    "            # -------------------------------\n",
    "            # Apply small \"cost of living\" penalty to encourage efficiency\n",
    "            reward -= 0.1\n",
    "            \n",
    "            # Reward progress for revealing new safe tiles\n",
    "            mine_hit = livesOld > game.lives\n",
    "            if not mine_hit:\n",
    "                newly_revealed = game.revealed_count - revealedOld\n",
    "                if newly_revealed > 0:\n",
    "                    reward += newly_revealed * 1.0\n",
    "\n",
    "            # Check for game termination\n",
    "            done = game.game_over or game.game_won\n",
    "            if done:\n",
    "                if game.game_won:\n",
    "                    reward += 100  # Large bonus for winning\n",
    "                else:\n",
    "                    reward += -100 # Large penalty for losing\n",
    "            \n",
    "            # ------------------\n",
    "            # Click Metric Update\n",
    "            # ------------------\n",
    "            if len(agent.CLICKS[\"left\"]) < agent.EPISODES:\n",
    "                agent.CLICKS[\"left\"].append(0)\n",
    "                agent.CLICKS[\"right\"].append(0)\n",
    "                agent.CLICKS[\"total\"].append(0)\n",
    "\n",
    "            agent.CLICKS[\"total\"][agent.EPISODES - 1] += 1\n",
    "            if actionType == 0:\n",
    "                agent.CLICKS[\"left\"][agent.EPISODES - 1] += 1\n",
    "            else:\n",
    "                agent.CLICKS[\"right\"][agent.EPISODES - 1] += 1\n",
    "\n",
    "            # Terminate if game is taking too long\n",
    "            if elapsedTime > 45:\n",
    "                game.game_over = True\n",
    "                done = True\n",
    "                if not game.game_won:\n",
    "                    # Ensure loss penalty is applied on timeout\n",
    "                    reward = -100\n",
    "                print(\"Game terminated: Time limit exceeded\")\n",
    "\n",
    "            # Reward clipping to prevent extreme values\n",
    "            reward = np.clip(reward, -100, 100)\n",
    "            \n",
    "            # Get Next State and Train Agent\n",
    "            stateNext = agent._encodeState()\n",
    "            loss, tdError = agent.train(stateCurrent, actionIdx, reward, stateNext, done)\n",
    "            \n",
    "            # Print progress for debugging\n",
    "            if agent.CLICKS[\"total\"][agent.EPISODES - 1] % 10 == 0 or done:\n",
    "                print(f\"[Ep {agent.EPISODES}, Step {agent.CLICKS['total'][agent.EPISODES - 1]}] \"\n",
    "                    f\"Score: {game.score} | Reward: {reward:.2f} | TD Error: {tdError:.2f} | \"\n",
    "                    f\"Action: ({x}, {y}) {'Reveal' if actionType == 0 else 'Flag'} | \"\n",
    "                    f\"Revealed: {game.revealed_count}/{GRID_SIZE * GRID_SIZE - game.total_mines}\")\n",
    "\n",
    "        # Reset game if episode is finished\n",
    "        if game.game_over or game.game_won:\n",
    "            draw_game(game, agent)\n",
    "            pygame.display.flip()\n",
    "            pygame.time.delay(1000)\n",
    "\n",
    "            # Episode Checkpoint\n",
    "            if agent.EPISODES % 100 == 0:\n",
    "                if len(agent.GAME_CONCLUSIONS) >= 300:\n",
    "                    last100WR = sum(agent.GAME_CONCLUSIONS[-100:]) / 100\n",
    "                    last200WR = sum(agent.GAME_CONCLUSIONS[-200:-100]) / 100\n",
    "                    last300WR = sum(agent.GAME_CONCLUSIONS[-300:-200]) / 100\n",
    "\n",
    "                    print(f\"\\n[Win Rate Trend]:\")\n",
    "                    print(f\"  Episodes {agent.EPISODES-300}-{agent.EPISODES-200}: {last300WR:.1%}\")\n",
    "                    print(f\"  Episodes {agent.EPISODES-200}-{agent.EPISODES-100}: {last200WR:.1%}\")\n",
    "                    print(f\"  Episodes {agent.EPISODES-100}-{agent.EPISODES}: {last100WR:.1%}\")\n",
    "\n",
    "                    if last100WR > last200WR:\n",
    "                        print(\"  ðŸŸ¢ Improving trend\")\n",
    "                    elif last100WR < last200WR:\n",
    "                        print(\"  ðŸŸ¡ Declining trend - consider hyperparameter adjustment\")\n",
    "                    else:\n",
    "                        print(\"  ðŸŸ¡ Flat trend - agent may be stuck\")\n",
    "\n",
    "            # Re-initialize the game and agent state for next episode\n",
    "            game = MinesweeperGame()\n",
    "            agent.endEpisode(game)\n",
    "\n",
    "            # ----------------------------\n",
    "            # Print Metrics\n",
    "            # ----------------------------\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Episode {agent.EPISODES - 1} finished: {'WIN' if agent.GAME_CONCLUSIONS[-1] else 'LOSS'} | Score: {agent.SCORES[-1]}\")\n",
    "            agent.printMetrics()\n",
    "            print(f\"{'='*60}\\n\")\n",
    "\n",
    "        return done\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.VIDEORESIZE:\n",
    "                # Handle window resize\n",
    "                SCREEN_WIDTH, SCREEN_HEIGHT = event.w, event.h\n",
    "                screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT), pygame.RESIZABLE)\n",
    "                \n",
    "                # Recalculate tile size\n",
    "                AVAILABLE_HEIGHT = SCREEN_HEIGHT - 230\n",
    "                AVAILABLE_WIDTH = SCREEN_WIDTH - 40\n",
    "                TILE_SIZE = min(AVAILABLE_HEIGHT // GRID_SIZE, AVAILABLE_WIDTH // GRID_SIZE)\n",
    "                \n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                clickHandler()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_r:\n",
    "                    game = MinesweeperGame()\n",
    "\n",
    "        # Let the agent take its turn\n",
    "        if agent is not None:\n",
    "            agentTurnLogic()\n",
    "\n",
    "        # Update hover state\n",
    "        mx, my = pygame.mouse.get_pos()\n",
    "        hoverX, hoverY = get_tile_from_mouse(mx, my)\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                game.grid[x][y].hover = (x == hoverX and y == hoverY)\n",
    "        \n",
    "        draw_game(game, agent)\n",
    "        pygame.display.flip()\n",
    "        clock.tick(60)\n",
    "    \n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from A2C import A2C\n",
    "\n",
    "def saveModel(modelName: str, agent: A2C):\n",
    "    \"\"\"\n",
    "    Saves the agent's model weights and training metrics to the './models' directory.\n",
    "\n",
    "    :param modelName: The name for the save file (e.g., \"run1\").\n",
    "    :type modelName: str\n",
    "    \n",
    "    :param agent: The A2C agent instance to save.\n",
    "    :type agent: A2C\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    modelsDir = \"./models\"\n",
    "    if not os.path.exists(modelsDir):\n",
    "        os.makedirs(modelsDir)\n",
    "    \n",
    "    # Define file paths\n",
    "    filePath = os.path.join(modelsDir, modelName)\n",
    "    weightsPath = f'{filePath}_model.weights.h5'\n",
    "    metricsPath = f'{filePath}_metrics.json'\n",
    "    \n",
    "    print(f\"\\nSaving progress to {filePath}...\")\n",
    "    \n",
    "    # Save the model's weights\n",
    "    agent.model.save_weights(weightsPath)\n",
    "    \n",
    "    # Prepare metrics data for saving\n",
    "    metricsData = {\n",
    "        \"EPISODES\": agent.EPISODES,\n",
    "        \"ELAPSED_TIME\": agent.ELAPSED_TIME,\n",
    "        \"GAME_CONCLUSIONS\": agent.GAME_CONCLUSIONS,\n",
    "        \"WIN_RATE\": agent.WIN_RATE,\n",
    "        \"HIGHEST_SCORE\": agent.HIGHEST_SCORE,\n",
    "        \"SCORES\": agent.SCORES,\n",
    "        \"TIME_EFFICIENCY\": agent.TIME_EFFICIENCY,\n",
    "        \"CLICKS\": agent.CLICKS,\n",
    "    }\n",
    "    \n",
    "    # Save metrics to a JSON file\n",
    "    with open(metricsPath, 'w') as f:\n",
    "        json.dump(metricsData, f, indent = 4)\n",
    "        \n",
    "    print(\"Progress saved successfully.\")\n",
    "\n",
    "def loadModel(modelName: str, agent: A2C) -> A2C:\n",
    "    \"\"\"\n",
    "    Loads the agent's model weights and training metrics from the './models' directory.\n",
    "\n",
    "    :param modelName: The name of the save file to load (e.g., \"run1\").\n",
    "    :type modelName: str\n",
    "    \n",
    "    :param agent: The A2C agent instance to load progress into.\n",
    "    :type agent: A2C\n",
    "    \n",
    "    :returns: The agent instance with loaded progress.\n",
    "    :rtype: A2C\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    filePath = os.path.join(\"./models\", modelName)\n",
    "    weightsPath = f'{filePath}_model.weights.h5'\n",
    "    metricsPath = f'{filePath}_metrics.json'\n",
    "    \n",
    "    # Check if both weights and metrics files exist\n",
    "    if os.path.exists(weightsPath) and os.path.exists(metricsPath):\n",
    "        print(f\"\\nLoading progress from {filePath}...\")\n",
    "        \n",
    "        # Load the model's weights\n",
    "        agent.model.load_weights(weightsPath)\n",
    "        \n",
    "        # Load metrics from the JSON file\n",
    "        with open(metricsPath, 'r') as f:\n",
    "            metricsData = json.load(f)\n",
    "        \n",
    "        # Populate the agent's attributes with the loaded data\n",
    "        agent.EPISODES = metricsData.get(\"EPISODES\", 1)\n",
    "        agent.ELAPSED_TIME = metricsData.get(\"ELAPSED_TIME\", [])\n",
    "        agent.GAME_CONCLUSIONS = metricsData.get(\"GAME_CONCLUSIONS\", [])\n",
    "        agent.WIN_RATE = metricsData.get(\"WIN_RATE\", 0.0)\n",
    "        agent.HIGHEST_SCORE = metricsData.get(\"HIGHEST_SCORE\", 0)\n",
    "        agent.SCORES = metricsData.get(\"SCORES\", [])\n",
    "        agent.TIME_EFFICIENCY = metricsData.get(\"TIME_EFFICIENCY\", 0.0)\n",
    "        agent.CLICKS = metricsData.get(\"CLICKS\", {\"left\": [], \"right\": [], \"total\": []})\n",
    "        \n",
    "        # Ensure the learning rate is correct for the loaded episode\n",
    "        if isinstance(agent.LR, dict):\n",
    "            currentLR = agent.optimizer.learning_rate.numpy()\n",
    "            newLR = currentLR\n",
    "            \n",
    "            # Find the latest episode milestone that has been passed\n",
    "            for milestone in sorted(agent.LR.keys()):\n",
    "                if agent.EPISODES >= milestone:\n",
    "                    newLR = agent.LR[milestone]\n",
    "            \n",
    "            # If the learning rate needs to be changed, update the optimizer\n",
    "            if newLR != currentLR:\n",
    "                print(f\"[LR Scheduler] Loaded at Episode {agent.EPISODES}: Setting learning rate to {newLR:.6f}\")\n",
    "                agent.optimizer.learning_rate.assign(newLR)\n",
    "        \n",
    "        print(\"Progress loaded successfully.\")\n",
    "    else:\n",
    "        print(\"\\nNo saved progress found. Starting a new training run.\")\n",
    "        \n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ec3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(\"20251011-run1\", agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
